{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table Of Content\n",
    " + Introduction\n",
    " + Gathering Data\n",
    " + Assessing Data\n",
    " + Cleaning Data\n",
    " \n",
    " \n",
    "### Introduction\n",
    " \n",
    ">Data wrangling is process of getting data ready for analysis and visualization. It is an iterative process with 3 stages. We can iterate through gathering,assessing and cleaning to get good quality data for analysis. So let's start with importing necessary libraries.\n",
    "\n",
    "> We are provided with three  main sources to gather data, But i'll be using an external API to fetch some data for my wrangling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In this step i gathered data from three given sources and three different methods. First source provided was a direct download link for our main data [Enhanced Twitter Archive](https://d17h27t6h515a5.cloudfront.net/topher/2017/August/59a4e958_twitter-archive-enhanced/twitter-archive-enhanced.csv). I downloaded it using browser.\n",
    "\n",
    "> Second source of data was [Image Predictions](https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv). which we were told to download using python. We used python's `requests` package to fetch data from udacity's servers and then saved it locally using python's file I/O functions. \n",
    "\n",
    "> Third source of our data was in form an API. I used Twitter's developer API to retrieve full information on tweets usind ids we're provided. First i had to create account on twitter developers. Then i generated Access token from Dashboard and saved them locally to access keys in my project.\n",
    "\n",
    "> Fetching data from API required and programming interface. I'm using python so I installed `Tweepy`, A python package to play around with Twitter API. Then I wrote code to fetch the data and saved it into `tweet_json.txt` ato access it later.\n",
    "\n",
    "> I also used [Dog API](https://dog.ceo/) to fetch all of the dog breeds which can be later used in cleaning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Assessing data means investigating the data and finding issues with data. Assessing can be done in two ways. Programatically and visually. Data issues can be devided into two types. One is Quality issues and other is Tidiness issues.\n",
    "\n",
    "> Assesing visually means opening dataset in some software and looking for problems in it. We can find issues like inaccurate data, incomplete data and tidiness issues visually data.\n",
    "\n",
    "> Assessing data programatically means using in built functions from python and finding issues. We can find issues like missing data and incorrect data types by assessing data programatically.\n",
    "\n",
    "> Issues in data are devided into quality issues and tidiness issues. Quality issues are related to quality of data like missing data, incorrect data types and incomplete data.\n",
    "\n",
    "> Tidiness issues are related with structure of data. You can read more about tidiness in [here](https://datacarpentry.org/organization-genomics/01-tidiness/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning\n",
    "\n",
    "> Cleaning data means fixing the issues we found during assessment. Cleaning data is mainly done programatically, but some data contains outliers which can not be fixed prgramatically so sometimes we need to fix them personally.\n",
    "\n",
    "> Cleaning is done in 3 steps. First we define our problems which means we take our issue and describe how we're going to fix that issue.\n",
    "\n",
    "> Second step is coding. We just code to fix the issue we described in our Define step. In this step i used python and pandas to fix data programatically. And third step is testing the solution. We check if our data issue is fixed or not."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
